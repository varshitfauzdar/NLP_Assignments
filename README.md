# NLP Foundations â€“ Practical Assignments

This repository contains hands-on Natural Language Processing (NLP) assignments implemented as part of my academic coursework.  
The primary objective of this repository is to build a **strong foundational understanding of text preprocessing, language modeling, normalization techniques, and syntactic analysis** using Python.

Each assignment focuses on understanding core NLP concepts through practical implementation.

---

## ðŸ“Œ Repository Overview

Natural Language Processing involves working with unstructured text data, which introduces challenges such as:

- Handling noisy and irregular text
- Extracting meaningful linguistic patterns
- Modeling word relationships probabilistically
- Measuring similarity between textual inputs
- Understanding grammatical structure

This repository demonstrates structured solutions to these challenges through progressively designed assignments.

---

# ðŸ“‚ Assignment 1 â€“ Tokenization & N-Gram Language Modeling

### ðŸ”¹ Implementations
- Text tokenization using **NLTK**
- Part-of-Speech (PoS) tagging using **spaCy**
- Text cleaning (lowercasing and punctuation removal)
- Construction of:
  - **Unigram Language Model**
  - **Bigram Language Model**
- Sentence generation using probabilistic N-gram models

### ðŸ”¹ Concepts Strengthened
- Text preprocessing pipelines
- Conditional probability for next-word prediction
- Statistical language modeling
- Limitations of N-gram models

---

# ðŸ“‚ Assignment 2 â€“ Tokenization Challenges & Edit Distance

### ðŸ”¹ Implementations
- Extraction of structured patterns using **Regular Expressions**
- Handling URLs, emails, hashtags, mentions, PAN numbers, mobile numbers
- Text normalization (repetitive character removal)
- Implementation of **Edit Distance (Levenshtein Distance)** using Dynamic Programming

### ðŸ”¹ Concepts Strengthened
- Real-world tokenization challenges
- Pattern-based text extraction
- String similarity measurement
- Application of dynamic programming in NLP

---

# ðŸ“‚ Assignment 3 â€“ Stemming & Lemmatization

### ðŸ”¹ Implementations
- Stemming using:
  - **Porter Stemmer**
  - **Lancaster Stemmer**
- Lemmatization using:
  - **WordNet Lemmatizer**
  - **spaCy Lemmatizer**
- Vocabulary size comparison before and after normalization
- Visualization of word reduction impact

### ðŸ”¹ Concepts Strengthened
- Word normalization techniques
- Aggressive vs conservative stemming
- Context-aware lemmatization
- Vocabulary reduction in NLP pipelines

---

# ðŸ“‚ Assignment 4 â€“ Part-of-Speech (PoS) Tagging

### ðŸ”¹ Problem Statement
Implement Part-of-Speech tagging to assign grammatical categories (noun, verb, adjective, etc.) to each word in a sentence.

### ðŸ”¹ Implementations
- Sentence tokenization
- Word tokenization
- PoS tagging using **NLTK**
- PoS tagging using **spaCy**
- Understanding Penn Treebank tag set
- Comparison between tagging approaches

### ðŸ”¹ Concepts Strengthened
- Syntactic structure of language
- Role of PoS tagging in NLP pipelines
- Differences between rule-based and statistical tagging
- Importance of grammar in downstream tasks (NER, parsing, sentiment analysis)

---

# ðŸ§  Overall Key Takeaways

- Text preprocessing is foundational to NLP systems
- Statistical models rely on probability rather than meaning
- Regex enables structured information extraction
- Word normalization reduces vocabulary complexity
- PoS tagging provides syntactic structure to text data

---

# ðŸ›  Technologies Used

- Python
- NLTK
- spaCy
- Regular Expressions
- Dynamic Programming
- Google Colab

---

# ðŸ“¦ Repository Structure

NLP-Foundations 
â”‚
â”œâ”€â”€ Assignment-1-N-GRAMS/
â”‚   â”œâ”€â”€ tokenization_pos_tagging.ipynb
â”‚   â”œâ”€â”€ unigram_bigram_language_model.ipynb
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ Assignment-2-EDIT_DISTANCE/
â”‚   â”œâ”€â”€ regex_tokenization.ipynb
â”‚   â”œâ”€â”€ edit_distance.ipynb
â”‚   â””â”€â”€ README.md
|
â”œâ”€â”€ Assignment-3-STEMMING_LEMMATIZATION/
â”‚ â”œâ”€â”€ stemming.ipynb
â”‚ â”œâ”€â”€ lemmatization.ipynb
â”‚ â”œâ”€â”€ large_text_processing.ipynb
â”‚ â””â”€â”€ README.md
|
â”œâ”€â”€ Assignment-4-RNN_CLASSIFICATION/
â”‚ â”œâ”€â”€ lstm_model.ipynb
â”‚ â”œâ”€â”€ gru_model.ipynb
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---
## ðŸ›  How to Run the Code

- All notebooks are implemented using **Google Colab**
- Required libraries are listed in `requirements.txt`
- Each notebook is self-contained and can be executed sequentially










---

## ðŸš€ ***THANKYOU***

